\label{sec:evaluation}

In this chapter, we focus on the evaluation of the proposed approach.
To do so we used the Goal-Question-Metric (GQM) evaluation methodology~\cite{basili_goal_1994}.

Our first evaluation goal G1 is to assess the feasibility of the approach. To do so, we need to evaluate if a software architect/developer can follow the proposed patterns to refine a goal model into components and artifacts. Also, we need to evaluate if the proposed planning algorithm is capable of autonomously creating a reliable deployment plan.
Such an evaluation required the definition of the following questions and metrics:

\begin{itemize}
  \item Q1.1: For the Filling Station Advisor case study, are the goal-component-artifact patterns a feasible approach to map artifacts from the CGM of the case study?
  \begin{itemize}
    \item Accurately maps artifacts for the Filling Station Advisor case study using proposed patterns.
  \end{itemize}

  \item Q1.2: How long would the algorithm take to come up with a deployment plan?
  \begin{itemize}
    \item Time to produce a plan.
  \end{itemize}

  \item Q1.3: How reliable would a plan provided
  by the algorithm be?
  \begin{itemize}
    \item Percentage of correct answers.
  \end{itemize}

\end{itemize}

Since the Filling Station Advisor has a limited size and does not allow for controlled factors experiments, our second goal G2 aims to provide a more comprehensible scalability evaluation of Goalp. So we defined the following questions and metrics:

\begin{itemize}
  \item Q2.1: How does the algorithm scale over the number of artifacts in the deployment plan?
  \begin{itemize}
    \item M2.1: The time consumed to come up with a deployment plan.
  \end{itemize}

  In the context of heterogeneity, we can have many artifacts in the repository that provide the same goal but with different context conditions.
  We named variability level the number of artifacts present in the repository that provide the same goal. It can affect the scalability of the planning because it leads the algorithm to verify alternative dependency trees, which can be computing intensive.

  \item Q2.2: How does the algorithm scale over the variability level on the repository?
  \begin{itemize}
    \item M2.2: The time consumed to come up with a deployment plan.
  \end{itemize}
\end{itemize}

\section{Feasibility Assessment}

We validated the feasibility of the approach applying it to the Filling Station Advisor.

The experiments were conducted using a laptop computer with Intel i5-3337U, 12GB DDR3 1600MHz memory, and Linux (Kernel 3.16.0-77generic). OracleJDK(1.8.0 91-b14) was used to build and run the project.
The experiments to evaluate the algorithm correctness
were implemented as automated tests under Java’s JUnit framework.

The code used to execute the evaluation is available on a public repository
\footnote{The evaluation experiments and step by step guide are available at:
\url{https://github.com/lesunb/goalp} Accessed on December 4th, 2016}
as well as the data obtained and scripts used to treat it.
\footnote{The dataset obtained and the R-Script\cite{the_r_foundation_r_2016} used to analyze the dataset is available at:
\url{https://github.com/lesunb/goalp-evaluation/tree/master/scalability/exp2} Accessed on December 4th, 2016}

\emph{Question 1.1, mapping components and artifacts }

We applied the patterns described in Table~\ref{table_cgm_to_components_patterns} to the CGM depicted in Figure~\ref{fig:goal_model_filling_station_advisor}. Then we defined the artifacts that would package that components following the proposed deployment architecture style (\ref{depl_arch_style}). We then mapped 21 different artifacts.

\emph{Question 1.2 and 1.3}

We instantiated an artifact repository with the mapped artifacts. We defined 7 deployment scenarios under different contexts. The scenarios that we used where: (s1) simple phone with ODB2, (s2) smartphone with ODB2, (s3) smartphone without car connection, (s4) dash computer with GPS and no nav sys integration and (s5) dash computer, connected, with GPS and navigation system integration. Scenarios (s6) dash computer without GPS and (s7) nav system without Internet connection or storage are scenarios for which there is no valid deployment plan.

\begin{figure*}[!htb]
 \centering
 \includegraphics[width=.8\linewidth]{case_study/comp_env_scearios}
 \caption{Computing Environment Evaluation Scenarios}
\label{fig:variability_scenarios}
\end{figure*}

\emph{Question 1.2:  How reliable would a plan provided
by the algorithm be?}: Test cases were created for each scenario (s1-s7).
To validate the algorithm’s correctness,
we verified the generated plans in each test case, asserting if the expected artifacts are in the resulting plan.
For scenarios s1-s5, the planning resulted in valid plans, with the correct artifacts. For scenarios s6 and s7, the algorithm returned \texttt{NULL}, as there is no possible deployment plan for these scenarios.

\begin{figure*}[!htb]
  \centering
  \epsfig{file=results/fsa_testcases, width=3.2in}
  \caption{Passing Tests}
\label{testcase}
\end{figure*}



\emph{Question 1.3: How long would the algorithm take to come up with a deployment plan?}: In each scenario, the time spent by the algorithm was measured. We executed the planning 100 time. Table~\ref{table:planning_time} shows the scenarios, the context, time spent for planning in each scenario, in mile-seconds together with standard deviation.

\begin{table}[!htb]
\centering
\caption{Time to come up with a plan}
\begin{tabular}{|p{0.7cm}|p{3.75cm}|p{2cm}|p{2cm}|}
\hline
  Ref. &
  Context &
  Time (ms) &
  Std \\ \hline

s1 &
C2, C4, C6, C9 &
12.28 ms & 30.69 \\ \hline
s2 &
C1, C3, C5, C8 &
6.24 ms & 16.22 \\ \hline
s3 &
C1, C5, C8 &
9.27 ms & 20.62\\ \hline
s4 &
C1, C3, C6, C10 &
9.01 ms & 20.94 \\ \hline
s5 &
C1, C3, C5, C7 &
6.83 ms & 17.18 \\ \hline
s6 &
C3, C6, C8 &
8.74 ms & 18.76 \\ \hline
s7 &
C1, C3, C7  &
6.44 ms & 17.51 \\ \hline

\end{tabular}
\label{table:planning_time}
\end{table}

    %
    % \item How does it scale over the amount of artifacts in the component repository?
    % \begin{itemize}
    %   \item time to plan the deployment in.
    %   \item space occupied during the planning.
    % \end{itemize}

\section{Scalability Assessment}

To evaluate the algorithm's scalability, we developed other test cases.  A repository with randomly generated artifacts was instantiated. And deployment requests that generate plans with a different number of artifacts were made. With this, we could evaluate the impact of the generated plan size in the planning time.
The generated repository had 143,500 artifacts.

The experiments were conducted using a virtual machine in the Azure Cloud. It was used an F1 instance, with 2.4 GHz Intel Xeon® E5-2673 v3 (Haswell) processor,
2GB DDR3 1600MHz memory, and Linux (Kernel 4.4.0-47-generic). OpenJDK(1.9 64bits-build 9) was used to build and run the project.

The code used to execute the evaluation is available on a public repository
\footnote{The needed source code and a step by step guide are available at
\url{https://github.com/lesunb/goalp/tree/master/scalability-evaluation} Accessed on December 4th, 2016}
as well as
the data obtained and scripts used to handle data and plot graphs.
\footnote{R Scripts\cite{the_r_foundation_r_2016} and used dataset are available at
\url{https://github.com/lesunb/goalp/tree/master/scalability-evaluation} Accessed on December 4th, 2016}

\emph{Q2.1: How does the algorithm scale over the number of artifacts in the deployment plan?} We executed 100 deployment planning requests, with different levels of complexity, where the generated plans were composed of artifacts summing from 40 to 3,100 artifacts. The experiment was repeated 10 times and the \emph{observed time} vs \emph{plan size} is shown in a boxplot graph in Figure~\ref{graph_plan_size_and_time}.

\begin{figure}[!htb]
  \centering
  \epsfig{file=results/planning/plan_size_vs_time.eps, width=5.2in}
  \caption{Scalability over the size of plan}
\label{graph_plan_size_and_time}
\end{figure}


\emph{Q2.2:  How does the algorithm scale over the variability level on the repository?}
We repeated the experiment for different levels of variability in the repository, from 1 to 10. A variability level of 1 being so that for each plan implementation there was just one artifact that implement the plan. While in variability level 2, for each plan implementation there was two artifacts, and so on. The experiment was repeated 10 times.

The average of the measures is depicted in Figure~\ref{graph_scalability}. Each curve represents a different level of variability.

\begin{figure}[!htb]
  \centering
  \epsfig{file=results/planning/size_and_variability.eps, width=5.2in}
  \caption{Scalability over variability level}
\label{graph_scalability}
\end{figure}

In the worst case, a deployment request that needs 3,100 artifacts, with 10 variants for each artifact, took less than 5s to be planned. Requests that required up to 1,000 artifacts could be fulfilled in less than half-second.

In conclusion, the time spent planning the deployment is expected to be negligible in face the time that would take to copy the artifacts from a repository to the target environment.

\section{Threats to validity}

We recognize some threats to the validity of the evaluation:

\emph{Construct validity:}
% Construct validity concerns establishing correct operational measures for the concepts being studied.
We used GQM methodology to proper design our experiments. We did an assumption that goal can be traced to implemented component and so the artifacts. The mapping of goals and plans to their concrete counterparts in the system architecture is a well-known problem of the requirements engineering community.
% We also did an assumption that we can restrict the deployment problem to the selection of artifacts, what could be an oversimplification. as deployment could involve other activities. Our assumption is that this other activities needed could be defined as plans, implemented and packaged as artifacts. 

\emph{Internal validity:}
% Internal validity concerns establishing a causal relationship, whereby certain conditions are shown to lead to other conditions.
The suitability of Goalp for deployment of Filling Station Advisor has been presented. The deployment planning result for the scenarios was validated.
In regard to scalability, we executed each experiment in a single resource and evaluated each time a single controlled variable.

\emph{External validity:}
%External validity concerns establishing the domain to which the findings can be generalized.
The scalability was evaluated for a randomly generated repository. For other repositories, the chains of dependencies could have different properties, which could change how the planning algorithm scale over the plan size.
Another threat is that the scalability evaluation was conducted in a cloud environment on a reasonably powerful machine. In other scenarios, we could have a much more limited machine in relation to processor power, memory size, network bandwidth, and battery. In that case, the planning could take longer.
%-boundary of the results,
%- limitations of the work
%- some criticism does make sense.
